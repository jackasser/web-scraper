# ウェブスクレイパー - GitHub Actions版

このリポジトリは、GitHub Actions を利用して特定のウェブサイトとその配下のページを一括取得するスクレイピングツールです。スクレイピング結果はGitHub Pagesで簡単に閲覧・共有できます。

## 📌 特徴

- **完全無料**: GitHub Actionsの無料枠で実行可能
- **簡単操作**: ブラウザから直接実行 (コマンドラインやローカル環境不要)
- **高度なカスタマイズ**: 
  - クロール深度の指定
  - 特定のファイルタイプのみ取得
  - URLパターンによる除外設定
- **自動UI生成**: 
  - インタラクティブな検索可能なWebインターフェース
  - ダウンロード可能なファイルリスト
  - スクレイピング詳細情報の表示
- **GitHub Pagesデプロイ**: 結果をWeb上で簡単に閲覧・共有

## 🚀 使い方

### 1. リポジトリの準備

1. このリポジトリを自分のGitHubアカウントに**フォーク**
2. リポジトリ設定 > Pagesで**GitHub Pages**を有効化
   - ソースに「gh-pages」ブランチを選択
   - 「保存」をクリック

### 2. スクレイピングの実行

1. リポジトリの「Actions」タブを開く
2. 「Web Scraper」ワークフローを選択
3. 「Run workflow」ボタンをクリック
4. 以下のパラメータを入力:
   - `target_url`: スクレイピング対象のURL 
     - 例: `https://example.com`
   - `max_depth`: クロールする深さ 
     - 推奨範囲: 1-5
     - デフォルト: 2
   - `file_types`: 取得するファイル形式 
     - カンマ区切り
     - 例: `html,pdf,jpg`
     - デフォルト: `html`
   - `exclude_patterns`: 除外するURLパターン 
     - カンマ区切りの正規表現
     - 例: `/blog/,/author/`
     - デフォルト: なし
5. 「Run workflow」をクリック

### 3. 結果の確認

スクレイピングが完了すると、以下のURLで結果を確認できます:
```
https://[あなたのユーザー名].github.io/[リポジトリ名]/
```

## ⚠️ 重要な注意事項

- **法的・倫理的配慮**:
  - ウェブサイトの利用規約を必ず確認
  - robots.txtを尊重
  - プライバシーとセンシティブ情報に注意
  - 著作権を遵守
- **サーバー負荷への配慮**:
  - 過度なリクエストを避ける
  - リクエスト間隔を適切に設定

## 🔧 カスタマイズ

より高度なカスタマイズが必要な場合は、以下のファイルを編集:
- `scraper.py`: スクレイピングロジック
- `create_ui.py`: UI生成ロジック
- `.github/workflows/scrape.yml`: GitHub Actionsワークフロー設定

## ⏱️ GitHub Actions使用制限

無料アカウントでの主な制限:
- 月間2,000分のActions実行時間
- 1ジョブあたり最大60分の実行時間
- 大規模スクレイピングでは制限に注意が必要

## 📄 ライセンス

このプロジェクトは**MITライセンス**の下で公開されています。

## 🤝 コントリビューション

改善や機能追加のプルリクエストを歓迎します！詳細は`CONTRIBUTING.md`を参照してください。

## 免責事項

このツールは教育・研究目的で提供されています。ウェブスクレイピングは法的・倫理的ガイドラインを遵守して利用してください。
