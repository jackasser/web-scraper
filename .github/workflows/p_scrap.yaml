name: Puppeteer Scraper

on:
  workflow_dispatch:
    inputs:
      target_url:
        description: 'スクレイピング対象のURL'
        required: true
        default: 'https://www.deepseek.com'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: |
          npm init -y
          npm install puppeteer

      - name: Create scraper script
        run: |
          cat > scraper.js << 'EOF'
          const puppeteer = require('puppeteer');
          const fs = require('fs');
          const path = require('path');
          
          async function scrape() {
            console.log('スクレイピングを開始します...');
            
            // 出力ディレクトリの作成
            const outputDir = 'scraped_data';
            if (!fs.existsSync(outputDir)) {
              fs.mkdirSync(outputDir, { recursive: true });
            }
            
            // ブラウザを起動
            const browser = await puppeteer.launch({
              headless: true,
              args: ['--no-sandbox', '--disable-setuid-sandbox']
            });
            
            try {
              const page = await browser.newPage();
              
              // ユーザーエージェントを設定
              await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36');
              
              // ページに移動
              await page.goto(process.argv[2], {
                waitUntil: 'networkidle2',
                timeout: 60000
              });
              
              console.log('ページを読み込みました');
              
              // スクリーンショットを保存
              await page.screenshot({
                path: path.join(outputDir, 'screenshot.png'),
                fullPage: true
              });
              
              // HTMLを保存
              const html = await page.content();
              fs.writeFileSync(path.join(outputDir, 'page.html'), html);
              
              // メタデータを抽出
              const metadata = await page.evaluate(() => {
                return {
                  title: document.title,
                  description: document.querySelector('meta[name="description"]')?.content || '',
                  links: Array.from(document.querySelectorAll('a')).map(a => ({
                    text: a.textContent.trim(),
                    href: a.href
                  })).filter(link => link.href.startsWith('http'))
                };
              });
              
              // メタデータを保存
              fs.writeFileSync(
                path.join(outputDir, 'metadata.json'), 
                JSON.stringify(metadata, null, 2)
              );
              
              console.log('スクレイピングが完了しました');
              console.log(`タイトル: ${metadata.title}`);
              console.log(`リンク数: ${metadata.links.length}`);
              
            } catch (error) {
              console.error('エラーが発生しました:', error);
            } finally {
              await browser.close();
            }
          }
          
          scrape();
          EOF

      - name: Run scraper
        run: node scraper.js ${{ github.event.inputs.target_url }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data
          path: scraped_data
          retention-days: 7
